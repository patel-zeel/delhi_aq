layers: [256, 256, 256, 256]
activation_scale: 30.0
iterations: 1000
batch_size: -1
lr: 1e-3